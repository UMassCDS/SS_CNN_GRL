{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import scipy.interpolate\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update dataset name as needed\n",
    "\n",
    "# EXPERIMENT = \"sand\"\n",
    "EXPERIMENT = \"clay\"\n",
    "\n",
    "# RAW_DATA_FOLDER_CLAY = '../'\n",
    "# RAW_MAT_FOLDER_CLAY = RAW_DATA_FOLDER_CLAY + 'raw_mat_new_clay/'\n",
    "# RAW_NPY_FOLDER_CLAY = RAW_DATA_FOLDER_CLAY + 'raw_npy_clay/'\n",
    "# NPY_FOLDER_CLAY = '../processed_input_data_clay/'\n",
    "\n",
    "RAW_DATA_FOLDER = '../'\n",
    "RAW_MAT_FOLDER = RAW_DATA_FOLDER + 'raw_mat_new_' + EXPERIMENT + '/'\n",
    "RAW_NPY_FOLDER = RAW_DATA_FOLDER + 'raw_npy_' + EXPERIMENT + '/'\n",
    "NPY_FOLDER = '../processed_input_data_' + EXPERIMENT + '/'\n",
    "\n",
    "#split data\n",
    "## attribute all exp_1 to training\n",
    "## attribute 2 windows (split_ratio) from exp_2 and exp_3 to training\n",
    "## the rest were randomly used for either eval and train. \n",
    "ML_exp = 'split_master'\n",
    "split_ratio=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXP_LIST contains a list of raw experiment XXX.mat files \n",
    "#Format: raw .mat --> raw .npy --> sliced & labeled .npy \n",
    "\n",
    "if EXPERIMENT == \"clay\":\n",
    "# Experiments list for Clay\n",
    "    EXP_LIST = ['EB_025_1','EB_025_2','EB_025_3',\n",
    "            'EB_050_1','EB_050_2','EB_050_3',\n",
    "            'EB_150_1','EB_150_2','EB_150_3',\n",
    "            'PP_025_1','PP_025_2',\n",
    "            'PP_050_1','PP_050_2',\n",
    "            'PP_100_1','PP_100_2',\n",
    "            'PP_150_1']\n",
    "else:\n",
    "# Experiments list for Sand\n",
    "    EXP_LIST = ['E355_64px_forCNN',\n",
    "    'E339_64px_forCNN',\n",
    "    'E280_64px_forCNN',\n",
    "    'E366_64px_forCNN',\n",
    "    'E342_64px_forCNN',\n",
    "    'E340_64px_forCNN',\n",
    "    'E364_64px_forCNN',\n",
    "    'E347_64px_forCNN',\n",
    "    'E346_64px_forCNN',\n",
    "    'E341_64px_forCNN',\n",
    "    'E365_64px_forCNN',\n",
    "    'E338_64px_forCNN',\n",
    "    'E345_64px_forCNN',\n",
    "    'E363_64px_forCNN',\n",
    "    'E368_64px_forCNN',\n",
    "    'E369_64px_forCNN',\n",
    "    'E458_64px_forCNN',\n",
    "    'E459_64px_forCNN',\n",
    "    'E463_64px_forCNN',\n",
    "    'E464_64px_forCNN',\n",
    "    'E499_64px_forCNN']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic functions to generate.npy files from mat files \n",
    "\n",
    "\n",
    "def create_npy(exp):\n",
    "    matlab_data_path = RAW_MAT_FOLDER + str(exp) + '.mat' \n",
    "    npy_data_path = RAW_NPY_FOLDER + str(exp) +'data.npy'\n",
    "    npy_label_path = RAW_NPY_FOLDER + str(exp) +'label.npy'\n",
    "    npy_para_path = RAW_NPY_FOLDER + str(exp) +'para.npy'\n",
    "    \n",
    "    if not os.path.exists(RAW_NPY_FOLDER):\n",
    "        os.mkdir(RAW_NPY_FOLDER)\n",
    "    if os.path.exists(npy_data_path):\n",
    "        raise Exception(\"raw .npy file already exist\")\n",
    "    else:  \n",
    "        data = sio.loadmat(matlab_data_path) [\"fault_map\"]  \n",
    "        label = sio.loadmat(matlab_data_path) [\"label\"]\n",
    "        \n",
    "        WW = sio.loadmat(matlab_data_path) [\"windowWidth\"]\n",
    "        overlap = sio.loadmat(matlab_data_path) [\"overlap\"]\n",
    "        \n",
    "        slice_param = [0 for a in range(4)]\n",
    "        slice_param[3] = WW[0,0]\n",
    "        slice_param[2] = 2* slice_param[3]\n",
    "        slice_param[1] = slice_param[3]- overlap[0,0]\n",
    "        slice_param[0] = 0\n",
    "        \n",
    "        # report sizes and save data, parameters and label\n",
    "        print(data.shape)\n",
    "        print(label.shape)\n",
    "        print(slice_param)\n",
    "        np.save(npy_data_path, data)\n",
    "        np.save(npy_label_path, label)\n",
    "        np.save(npy_para_path, slice_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EB_025_1\n",
      "(128, 264, 227)\n",
      "(5, 227, 2)\n",
      "[0, 50, 128, 64]\n",
      "EB_025_2\n",
      "(128, 264, 227)\n",
      "(5, 227, 2)\n",
      "[0, 50, 128, 64]\n",
      "EB_025_3\n",
      "(128, 264, 227)\n",
      "(5, 227, 2)\n",
      "[0, 50, 128, 64]\n",
      "EB_050_1\n",
      "(128, 264, 218)\n",
      "(5, 218, 2)\n",
      "[0, 50, 128, 64]\n",
      "EB_050_2\n",
      "(128, 264, 219)\n",
      "(5, 219, 2)\n",
      "[0, 50, 128, 64]\n",
      "EB_050_3\n",
      "(128, 264, 220)\n",
      "(5, 220, 2)\n",
      "[0, 50, 128, 64]\n",
      "EB_150_1\n",
      "(128, 264, 192)\n",
      "(5, 192, 2)\n",
      "[0, 50, 128, 64]\n",
      "EB_150_2\n",
      "(128, 264, 193)\n",
      "(5, 193, 2)\n",
      "[0, 50, 128, 64]\n",
      "EB_150_3\n",
      "(128, 264, 191)\n",
      "(5, 191, 2)\n",
      "[0, 50, 128, 64]\n",
      "PP_025_1\n",
      "(128, 264, 227)\n",
      "(5, 227, 2)\n",
      "[0, 50, 128, 64]\n",
      "PP_025_2\n",
      "(128, 264, 228)\n",
      "(5, 228, 2)\n",
      "[0, 50, 128, 64]\n",
      "PP_050_1\n",
      "(128, 264, 219)\n",
      "(5, 219, 2)\n",
      "[0, 50, 128, 64]\n",
      "PP_050_2\n",
      "(128, 264, 218)\n",
      "(5, 218, 2)\n",
      "[0, 50, 128, 64]\n",
      "PP_100_1\n",
      "(128, 264, 190)\n",
      "(5, 190, 2)\n",
      "[0, 50, 128, 64]\n",
      "PP_100_2\n",
      "(128, 264, 205)\n",
      "(5, 205, 2)\n",
      "[0, 50, 128, 64]\n",
      "PP_150_1\n",
      "(128, 264, 204)\n",
      "(5, 204, 2)\n",
      "[0, 50, 128, 64]\n"
     ]
    }
   ],
   "source": [
    "# Generate and save raw_npy for list of experiment\n",
    "# Only need to run once! \n",
    "\n",
    "for e in EXP_LIST:\n",
    "    print(e)\n",
    "    create_npy(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function creates slices of each fault map\n",
    "def slicing_hw(RAW_NPY_FOLDER, exp): \n",
    "    npy_data_path = RAW_NPY_FOLDER + str(exp) +'data.npy'\n",
    "    npy_para_path = RAW_NPY_FOLDER + str(exp) +'para.npy'\n",
    "    \n",
    "    data = np.load(npy_data_path)\n",
    "    print(data.shape)\n",
    "    slice_param = np.load(npy_para_path)\n",
    "    \n",
    "    #data has dimension of (x, y, cfuv, t)\n",
    "    width_tot = data.shape[1]\n",
    "    t_tot = data.shape[2]\n",
    "    #print(width_tot,t_tot)\n",
    "    \n",
    "    #unpack slice_param into initial_index, strike_lenght, window_H and window_W\n",
    "    ii = slice_param[0]\n",
    "    strike = slice_param[1]\n",
    "    H = slice_param[2]\n",
    "    W = slice_param[3]\n",
    "    print(ii,strike,H,W)\n",
    "    \n",
    "    # set number of windows and slice data and label \n",
    "    num_win = 1 + (width_tot - W)//strike   \n",
    "    \n",
    "    data_hw = []\n",
    "    w_idx = np.arange(num_win)\n",
    "    for w_i in w_idx:\n",
    "        left_win = w_i*strike\n",
    "        #print(left_win)\n",
    "        data_hwi = data[:,left_win:left_win+W]\n",
    "        #data_hw is a list of tuples (number of tuples = num_win)\n",
    "        #first element of tuple has a shape = (WindowHeight, WindowWidth, numFrames)\n",
    "        #second element of tuple is the w_idx\n",
    "        data_hw.append((data_hwi, w_i))\n",
    "\n",
    "    print(\"Data_slice_shape =\", data_hw[0][0].shape)\n",
    "    return data_hw, num_win, t_tot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function gathers the label information and saves with slices to be read for dividing into \n",
    "#training, eval and testing data sets.\n",
    "def data_slice_KE(RAW_NPY_FOLDER, NPY_FOLDER, exp):\n",
    "    \n",
    "    #set file location for \n",
    "    npy_exp_folder_path = NPY_FOLDER + 'slice_npy/' + str(exp)\n",
    "    master_folder_path = NPY_FOLDER + 'file_master/'\n",
    "    \n",
    "    #need to grab the KE labels\n",
    "    npy_label_path = RAW_NPY_FOLDER + str(exp) +'label.npy'\n",
    "    label = np.load(npy_label_path)\n",
    "    #print('label', label.shape)\n",
    "    \n",
    "    if os.path.exists(npy_exp_folder_path):\n",
    "        raise Exception(\"slice_exp.npy already exist\")\n",
    "    else: \n",
    "        os.makedirs(npy_exp_folder_path)\n",
    "        if not os.path.exists(master_folder_path): \n",
    "            os.makedirs(master_folder_path)     \n",
    "    \n",
    "    #slice the fault_map\n",
    "    data_hw, num_win, t_tot = slicing_hw(RAW_NPY_FOLDER, exp)\n",
    "    \n",
    "    w_idx = np.arange(num_win)\n",
    "    file_master = []\n",
    "    \n",
    "    #looping through all windows and times \n",
    "    for w_i in w_idx: \n",
    "        for t_i in range(t_tot):\n",
    "            #unpacking data_hw\n",
    "            data_f, win = data_hw[w_i]\n",
    "            #print(win)\n",
    "        \n",
    "            #only process slice with non-zero fault traces\n",
    "            if np.sum(data_f[:,:,t_i]) != 0:\n",
    "                \n",
    "                #The first array of the label array are the KE and the second array is the std\n",
    "                label_KE = label[w_i,t_i,0]\n",
    "                label_SD = label[w_i,t_i,1]\n",
    "                #print(w_i,t_i,label_KE,label_SD)\n",
    "                \n",
    "                name_path = '/' + str(label_KE) + '_' + str(label_SD) + '_win_' + str(\n",
    "                        win) + '_t_' + str('{:03d}'.format(t_i)) + '_' + str(exp) + '_cfuv.npy'\n",
    "                # save sliced and labeled dataset in slice_npy folder\n",
    "                np.save(npy_exp_folder_path + name_path, data_f[:,:,t_i])\n",
    "                \n",
    "                # add to file master\n",
    "                file_master.append(str(exp) + name_path)          \n",
    "    \n",
    "    master_data_path = master_folder_path + str(exp) + \".txt\"\n",
    "    print('non_zero_slice =', len(file_master))\n",
    "    np.savetxt(master_data_path, file_master, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 264, 227)\n",
      "0 50 128 64\n",
      "Data_slice_shape = (128, 64, 227)\n",
      "non_zero_slice = 927\n",
      "(128, 264, 227)\n",
      "0 50 128 64\n",
      "Data_slice_shape = (128, 64, 227)\n",
      "non_zero_slice = 1005\n",
      "(128, 264, 227)\n",
      "0 50 128 64\n",
      "Data_slice_shape = (128, 64, 227)\n",
      "non_zero_slice = 931\n",
      "(128, 264, 218)\n",
      "0 50 128 64\n",
      "Data_slice_shape = (128, 64, 218)\n",
      "non_zero_slice = 892\n",
      "(128, 264, 219)\n",
      "0 50 128 64\n",
      "Data_slice_shape = (128, 64, 219)\n",
      "non_zero_slice = 871\n",
      "(128, 264, 220)\n",
      "0 50 128 64\n",
      "Data_slice_shape = (128, 64, 220)\n",
      "non_zero_slice = 957\n",
      "(128, 264, 192)\n",
      "0 50 128 64\n",
      "Data_slice_shape = (128, 64, 192)\n",
      "non_zero_slice = 649\n",
      "(128, 264, 193)\n",
      "0 50 128 64\n",
      "Data_slice_shape = (128, 64, 193)\n",
      "non_zero_slice = 673\n",
      "(128, 264, 191)\n",
      "0 50 128 64\n",
      "Data_slice_shape = (128, 64, 191)\n",
      "non_zero_slice = 644\n",
      "(128, 264, 227)\n",
      "0 50 128 64\n",
      "Data_slice_shape = (128, 64, 227)\n",
      "non_zero_slice = 912\n",
      "(128, 264, 228)\n",
      "0 50 128 64\n",
      "Data_slice_shape = (128, 64, 228)\n",
      "non_zero_slice = 915\n",
      "(128, 264, 219)\n",
      "0 50 128 64\n",
      "Data_slice_shape = (128, 64, 219)\n",
      "non_zero_slice = 825\n",
      "(128, 264, 218)\n",
      "0 50 128 64\n",
      "Data_slice_shape = (128, 64, 218)\n",
      "non_zero_slice = 852\n",
      "(128, 264, 190)\n",
      "0 50 128 64\n",
      "Data_slice_shape = (128, 64, 190)\n",
      "non_zero_slice = 711\n",
      "(128, 264, 205)\n",
      "0 50 128 64\n",
      "Data_slice_shape = (128, 64, 205)\n",
      "non_zero_slice = 757\n",
      "(128, 264, 204)\n",
      "0 50 128 64\n",
      "Data_slice_shape = (128, 64, 204)\n",
      "non_zero_slice = 766\n"
     ]
    }
   ],
   "source": [
    "# Only need to run once! \n",
    "for exp in EXP_LIST:\n",
    "    data_slice_KE(RAW_NPY_FOLDER, NPY_FOLDER, exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(DATA_FOLDER,ML_exp, G1, G2, split_ratio):\n",
    "    train_combine = []\n",
    "    eval_combine = []\n",
    "    test_combine = []\n",
    "    eval_exp_win_stat = []\n",
    "    test_exp_win_stat = []\n",
    "    train_exp_win_stat = []\n",
    "    for exp in G1:        \n",
    "        fmt = DATA_FOLDER + 'file_master/' + str(exp) + '.txt'\n",
    "        a = np.loadtxt(fmt, dtype=str)       \n",
    "        # for each experiments in G1, we 100% attribute them to training dataset. \n",
    "        train_combine = np.concatenate([train_combine, a])\n",
    "        # housekeeping for the exp/slice included in training\n",
    "        num_win = int(a[-1].split('win_')[-1][0:1])+1\n",
    "        for i in range (num_win):\n",
    "            train_slice = [exp, i]\n",
    "            train_exp_win_stat.append(train_slice)\n",
    "            \n",
    "    for exp in G2:\n",
    "        fmt = DATA_FOLDER + 'file_master/' + str(exp) + '.txt'\n",
    "        b = np.loadtxt(fmt, dtype=str)\n",
    "        #check last element for its window number + 1 to get #of window\n",
    "        num_win = int(b[-1].split('win_')[-1][0:1])+1\n",
    "        #check how many time slice per window \n",
    "        win_len = len(b)/num_win\n",
    "        print(\"num_win\",num_win)\n",
    "        print(\"len_b\",len(b))\n",
    "        #check randomized which slice would be included in \n",
    "        shf_idx = np.arange(num_win)\n",
    "        print(shf_idx)\n",
    "        np.random.shuffle(shf_idx)\n",
    "        print(shf_idx)\n",
    "        \n",
    "        # split_eval = split_ratio\n",
    "        split_eval = num_win//3\n",
    "        \n",
    "        #distribute one random window-slice to eval\n",
    "        for idx in range(split_eval):\n",
    "            eval_ini = int(shf_idx[idx]*win_len)\n",
    "            eval_end = int((shf_idx[idx]+1)*win_len)\n",
    "            eval_combine = np.concatenate([eval_combine,  b[eval_ini:eval_end]])\n",
    "            # housekeeping for the exp/slice included in evaluating\n",
    "            eval_slice = [exp,shf_idx[idx]] \n",
    "            eval_exp_win_stat.append(eval_slice)\n",
    "                \n",
    "        #distribute one random window-slice to test\n",
    "        test_ini = int(shf_idx[split_eval]*win_len)\n",
    "        test_end = int((shf_idx[split_eval]+1)*win_len)\n",
    "        test_combine = np.concatenate([test_combine,  b[test_ini:test_end]])\n",
    "        # houseckeeping for the exp/slice included in testting\n",
    "        test_slice = [exp,shf_idx[split_eval]] \n",
    "        test_exp_win_stat.append(test_slice)\n",
    "        \n",
    "        #distribute the rest of random window-slices to train\n",
    "        for idx in range(split_eval+1,num_win):\n",
    "            train_ini = int(shf_idx[idx]*win_len)\n",
    "            train_end = int((shf_idx[idx]+1)*win_len)\n",
    "            train_combine = np.concatenate([train_combine, b[train_ini:train_end]])\n",
    "            # houseckeeping for the exp/slice included in training\n",
    "            train_slice = [exp,shf_idx[idx]] \n",
    "            train_exp_win_stat.append(train_slice)\n",
    "            \n",
    "    tot_dataset = len(train_combine) + len(eval_combine) + len(test_combine)\n",
    "    train_ratio = format(len(train_combine)/tot_dataset, '.2f')\n",
    "    eval_ratio = format(len(eval_combine)/tot_dataset, '.2f')\n",
    "    test_ratio = format(len(test_combine)/tot_dataset, '.2f')\n",
    "    \n",
    "    stat_dict = {'train_ew': train_exp_win_stat,\n",
    "                'eval_ew': eval_exp_win_stat,\n",
    "                'test_ew': test_exp_win_stat,\n",
    "                'tot_dataset':tot_dataset,\n",
    "                'train_ratio':train_ratio,\n",
    "                'eval_ratio':eval_ratio,\n",
    "                'test_ratio':test_ratio}\n",
    "    print(stat_dict)\n",
    "    ML_EXP_PATH = DATA_FOLDER + str(ML_exp)\n",
    "    if not os.path.exists(ML_EXP_PATH): \n",
    "        os.makedirs(ML_EXP_PATH)\n",
    "    \n",
    "    with open(ML_EXP_PATH +'/data_stat.txt', 'w') as f:\n",
    "        print(stat_dict, file=f)\n",
    "        \n",
    "    train_data_path = ML_EXP_PATH +  '/train_master.txt'\n",
    "    np.savetxt(train_data_path, train_combine, fmt=\"%s\")\n",
    "    \n",
    "    eval_data_path = ML_EXP_PATH +  '/eval_master.txt'\n",
    "    np.savetxt(eval_data_path, eval_combine, fmt=\"%s\")\n",
    "    \n",
    "    test_data_path = ML_EXP_PATH +  '/test_master.txt'\n",
    "    np.savetxt(test_data_path, test_combine, fmt=\"%s\")\n",
    "    return stat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_win 5\n",
      "len_b 1005\n",
      "[0 1 2 3 4]\n",
      "[3 1 0 2 4]\n",
      "num_win 5\n",
      "len_b 931\n",
      "[0 1 2 3 4]\n",
      "[4 1 0 2 3]\n",
      "num_win 5\n",
      "len_b 871\n",
      "[0 1 2 3 4]\n",
      "[1 4 0 3 2]\n",
      "num_win 5\n",
      "len_b 957\n",
      "[0 1 2 3 4]\n",
      "[1 3 2 4 0]\n",
      "num_win 5\n",
      "len_b 673\n",
      "[0 1 2 3 4]\n",
      "[0 1 2 3 4]\n",
      "num_win 5\n",
      "len_b 644\n",
      "[0 1 2 3 4]\n",
      "[0 1 4 3 2]\n",
      "num_win 5\n",
      "len_b 915\n",
      "[0 1 2 3 4]\n",
      "[3 0 2 1 4]\n",
      "num_win 5\n",
      "len_b 852\n",
      "[0 1 2 3 4]\n",
      "[0 4 1 2 3]\n",
      "num_win 5\n",
      "len_b 757\n",
      "[0 1 2 3 4]\n",
      "[1 3 0 4 2]\n",
      "{'train_ew': [['EB_025_1', 0], ['EB_025_1', 1], ['EB_025_1', 2], ['EB_025_1', 3], ['EB_025_1', 4], ['EB_050_1', 0], ['EB_050_1', 1], ['EB_050_1', 2], ['EB_050_1', 3], ['EB_050_1', 4], ['EB_150_1', 0], ['EB_150_1', 1], ['EB_150_1', 2], ['EB_150_1', 3], ['EB_150_1', 4], ['PP_025_1', 0], ['PP_025_1', 1], ['PP_025_1', 2], ['PP_025_1', 3], ['PP_025_1', 4], ['PP_050_1', 0], ['PP_050_1', 1], ['PP_050_1', 2], ['PP_050_1', 3], ['PP_050_1', 4], ['PP_100_1', 0], ['PP_100_1', 1], ['PP_100_1', 2], ['PP_100_1', 3], ['PP_100_1', 4], ['EB_025_2', 0], ['EB_025_2', 2], ['EB_025_2', 4], ['EB_025_3', 0], ['EB_025_3', 2], ['EB_025_3', 3], ['EB_050_2', 0], ['EB_050_2', 3], ['EB_050_2', 2], ['EB_050_3', 2], ['EB_050_3', 4], ['EB_050_3', 0], ['EB_150_2', 2], ['EB_150_2', 3], ['EB_150_2', 4], ['EB_150_3', 4], ['EB_150_3', 3], ['EB_150_3', 2], ['PP_025_2', 2], ['PP_025_2', 1], ['PP_025_2', 4], ['PP_050_2', 1], ['PP_050_2', 2], ['PP_050_2', 3], ['PP_100_2', 0], ['PP_100_2', 4], ['PP_100_2', 2]], 'eval_ew': [['EB_025_2', 3], ['EB_025_3', 4], ['EB_050_2', 1], ['EB_050_3', 1], ['EB_150_2', 0], ['EB_150_3', 0], ['PP_025_2', 3], ['PP_050_2', 0], ['PP_100_2', 1]], 'test_ew': [['EB_025_2', 1], ['EB_025_3', 1], ['EB_050_2', 4], ['EB_050_3', 3], ['EB_150_2', 1], ['EB_150_3', 1], ['PP_025_2', 0], ['PP_050_2', 4], ['PP_100_2', 3]], 'tot_dataset': 12521, 'train_ratio': '0.76', 'eval_ratio': '0.12', 'test_ratio': '0.12'}\n"
     ]
    }
   ],
   "source": [
    "if EXPERIMENT == \"clay\":\n",
    "    G1 = ['EB_025_1', 'EB_050_1', 'EB_150_1','PP_025_1','PP_050_1','PP_100_1']\n",
    "    G2 = ['EB_025_2', 'EB_025_3', 'EB_050_2','EB_050_3','EB_150_2','EB_150_3','PP_025_2','PP_050_2','PP_100_2' ]\n",
    "    create_dataset(NPY_FOLDER, ML_exp, G1, G2,split_ratio)\n",
    "else:\n",
    "    G1 = ['E280_64px_forCNN',\n",
    "    'E339_64px_forCNN',\n",
    "    'E347_64px_forCNN',\n",
    "    'E365_64px_forCNN',\n",
    "    'E459_64px_forCNN']\n",
    "\n",
    "    G2 = ['E338_64px_forCNN',\n",
    "    'E341_64px_forCNN',\n",
    "    'E342_64px_forCNN',\n",
    "    'E345_64px_forCNN',\n",
    "    'E346_64px_forCNN',\n",
    "    'E355_64px_forCNN',\n",
    "    'E364_64px_forCNN',\n",
    "    'E366_64px_forCNN',\n",
    "    'E369_64px_forCNN',\n",
    "    'E363_64px_forCNN',\n",
    "    'E368_64px_forCNN',\n",
    "    'E458_64px_forCNN',\n",
    "    'E463_64px_forCNN',\n",
    "    'E464_64px_forCNN',\n",
    "    'E499_64px_forCNN']\n",
    "    create_dataset(NPY_FOLDER, ML_exp, G1, G2,split_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5b869f9e4e23c26c27a4c0697de3e6b12f69bb347bea3ec85fb04f0bb147111"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('nn682')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
